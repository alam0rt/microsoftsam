# Mumble Voice Bot Configuration
# Copy to config.yaml and customize for your setup
# Environment variables can be used with ${VAR_NAME} syntax

llm:
  # OpenAI-compatible endpoint (works with Ollama, vLLM, OpenAI, etc.)
  endpoint: "http://localhost:11434/v1/chat/completions"
  model: "llama3.2:3b"
  api_key: "${LLM_API_KEY}"  # Optional, set via environment
  
  # Prompt configuration (choose one):
  # Option 1: Inline system prompt
  system_prompt: |
    You are a helpful voice assistant in a Mumble voice chat.
    Keep responses concise and conversational (1-3 sentences).
    Be friendly but not overly verbose - this is voice, not text.
  
  # Option 2: Load prompt from file (overrides system_prompt)
  # prompt_file: "prompts/default.md"
  
  # Option 3: Add a personality on top of the prompt
  # personality: "imperial"  # Loads personalities/imperial.md
  
  timeout: 30.0
  # max_tokens: 256  # Optional, limit response length
  # temperature: 0.7  # Optional, control randomness

tts:
  ref_audio: "reference.wav"  # Reference audio for voice cloning
  ref_duration: 5.0           # Seconds of reference to use
  num_steps: 4                # Quality vs speed (3-4 recommended)
  speed: 1.0                  # Playback speed

mumble:
  host: "localhost"
  port: 64738
  user: "VoiceBot"
  password: null
  channel: null

bot:
  wake_word: null             # e.g., "hey bot" - if null, responds to all speech
  silence_threshold_ms: 1500  # Silence before processing speech
  max_recording_ms: 30000     # Max speech duration
  asr_threshold: 2000         # RMS threshold for voice activity
  enable_conversation: true   # Enable LLM conversation mode
  conversation_timeout: 300   # Clear history after 5 minutes of inactivity
  max_response_staleness: 5.0 # Skip responses older than this (increase for slow LLM)

# Speech-to-Text configuration
stt:
  # Provider options: "local", "wyoming", "sherpa_nemotron", "nemotron_nemo"
  provider: "local"           # Default: use local Whisper via LuxTTS
  
  # Wyoming settings (for provider: "wyoming")
  wyoming_host: null          # e.g., "localhost" for wyoming-faster-whisper
  wyoming_port: 10300
  
  # Sherpa-onnx Nemotron settings (for provider: "sherpa_nemotron")
  # Requires sherpa-onnx installed and model files downloaded
  # sherpa_encoder: "/path/to/encoder.onnx"
  # sherpa_decoder: "/path/to/decoder.onnx"
  # sherpa_joiner: "/path/to/joiner.onnx"
  # sherpa_tokens: "/path/to/tokens.txt"
  # sherpa_provider: "cuda"   # or "cpu"
  
  # NeMo Nemotron settings (for provider: "nemotron_nemo")
  # Requires nemo_toolkit installed
  nemotron_model: "nvidia/nemotron-speech-streaming-en-0.6b"
  nemotron_chunk_ms: 160      # 80=fastest, 160=balanced, 560/1120=accurate
  nemotron_device: "cuda"

# Model storage paths (optional)
# Configure where HuggingFace and PyTorch models are downloaded/cached
# Useful for systems with limited home directory space or shared model caches
models:
  # Use .hf-cache/ in the repo (gitignored) for local development:
  # hf_home: ".hf-cache"
  
  # Or use a shared cache directory:
  # hf_home: "/data/models/huggingface"     # HF_HOME - main HuggingFace directory
  # hf_hub_cache: "/data/models/hf/hub"     # HF_HUB_CACHE - downloaded model files
  # transformers_cache: "/data/models/transformers"  # TRANSFORMERS_CACHE - legacy
  # torch_home: "/data/models/torch"        # TORCH_HOME - PyTorch pretrained models
  # xdg_cache_home: "/data/cache"           # XDG_CACHE_HOME - Linux fallback cache
